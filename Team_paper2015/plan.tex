 
\section{Our focus and plan}

% % % TODO % % % %
%from rules: Innovative technology (if any) TODO describe usage of STRANDS
%from rules: Reusability of the system or parts thereof - see STRANDS
This section summarises our intentions for our system this year. We would like to aim for high re-usability of code in our robotic system and to demonstrate that the current state of the art in AI algorithms mixed with our extensions can be successfully used to produce a robust, effective and complete robotic system with domestic applications.  

\subsection{Catering for Granny Annieâ€™s Comfort}

A robot will receive commands from Granny Annie by natural speech. These commands may contain interaction with an intelligent flat or bringing to her a specific object. 
We have not participated in this task last year because we had not a robust speech recognition as neither of us is an expert in the field. 
However, we have started a cooperation with research group in this field and we hope to be able to recognise speech more robustly than using CMU Sphinx last year, see Sec.~\ref{sec:speechrec}.

We would like to prepare our robot so that it can perform the first part - cooperation/interaction with an intelligent flat. This subtask still contains interesting challenges, such as robust speech recognition and robot navigation in a way, which is comfortable and natural for a human.  

%The problem of speech recognition is often discussed within the robotic community. It is something, which is necessary for future robots to cooperate with humans. Assuming a robot is working at Granny's flat, the robot can learn and adapt to her voice, which will increases its performance over time. Moreover, Granny is living alone, thus, it can be assumed that the level of background noise is minimal. In contrast, during the competition, speech recognition provides a lot of challenges. The background noise is really high during competition because of the spectators. Furthermore, the person playing Granny Annie changes, hence a robot cannot improve its behaviour. 



%The second part of this task - bringing someone a specific object - is even more challenging as it includes two large robotics domains, computer vision and manipulation. As our robot has no manipulator this year (see Section \ref{sec:hardware}), we might be able to perform only object search. We might benefit from experiences from a previous project in our robotic group - CogX \cite{cogx}. In this project, one of the robot's abilities was to search for known objects by taking into account common sense knowledge (i.e. prior knowledge) in the form of probabilities of objects' presence in different rooms. However, this will still require a lot of effort from us, as the robot was not using ROS before and we will need to understand and integrate bits into our system. We assume to work on this part next year.

\subsection{Welcoming visitors}

In this task, a robot should welcome visitors, recognise them and accompany them to a specific location in Granny's flat. Several components are needed in this task: computer vision for face recognition and for uniform detection. Moreover, machine learning techniques are needed to be able to learn these two patterns in order to recognise them. A robot can also benefit from speech recognition if face/uniform recognition does not provide a certain decision. Additionally, a robot must have robust navigation in an environment in order to accompany a person to a specific place. 
Finally, human-robot interaction is needed while accompanying a person.

Last year, we successfully completed this task. Our system was built on several components which are mentioned in Sec.~\ref{sec:software}. 
Here, we would like to discuss their properties, advantages and disadvantages and their future extensions. 
Our core state machine is built for this task calling several components. 
We do not plan changes in our face recognition system as neither of us is an expert in this field. 
However, we will create bigger database of faces in order to support machine learning algorithm.

Uniform recognition needs to be improved due to following reasons.
First, recognition was based on manual sensitive calibration of colours last year, which slowed down preparation and the calibration was sometimes impossible. 
Second, uniform detection was based only on segmentation of all the picture. This means, if there is significantly big object of certain colour belonging to an uniform (white/yellow) in the background, this object will be classified as the person. 
Both issue were limitations last year and we are improving them.  

The last issue of our system last year was not sufficient detection of a human. Thus, our human-robot interaction was strongly limited. 
We were using only a leg detector based on the readings from the laser scanner. 
This system was not robust enough. 
Hence we are testing how to combine this system with an upper-body detector based on a depth camera or an IR camera.

\subsection{Getting to know my home}

In this task, a robot should recognise changes in a flat, such as open/closed doors, furniture has been moved to another room or everyday objects were moved. 
The robot can detect these changes autonomously or by cooperation with a human who used speech or gestures.
After detection, the robot is asked to use this new knowledge to execute some commands, such as to bring a mug on the dining table which is now in living room.

The state machine for this task from last year is in Fig.~\ref{fig:st1}.
Our system was built on two parts - autonomous detection of doors using the laser rangefinder and cooperation with human in order to detect where is furniture. 
 However, the robot was unable to detect exact position of the furniture which is important to be able to use the knowledge later. 
Therefore, we would like to integrate one of the STRANDS packages this year, which is able to detect dynamic clusters from 3D point clouds and recognise to which object they belong. 
%TODO citation
Additionally, we would like to detect objects as well, see Sec.~\ref{sec:objectbenchm}.

Finally, we used CMU Sphinx last year for speech recognition. 
However, this system is not proper for this usage due to limited dictionary and usage. 
Therefore, we are going to change our speech recognition system, see Sec.~\ref{sec:speechrec}.

\begin{figure}[!htb]
\centering
\includegraphics[width=3.in]{statemachine_t1.png}
\caption{The state machine for task Getting to know my home}
\label{fig:st1}
\end{figure}

\subsection{Object recognition benchmarking} %TODO name?

\subsection{Navigation functionality benchmark}

\subsection{Speech recognition functionality benchmark}

\subsection{Timetable}

We are doing our best in order to participate in all tests. However, our priorities are following.
First, we will perform complete \textit{Welcoming visitor} task (referred as MW). 
Moreover, we will detect not only doors changes, but also precise locations of furniture in \textit{Getting to know my home}(referred as MG) task. 
In \textit{}

 In order to achieve these goals, we set up our milestones:
\begin{itemize}
\item MW1 - training of a face, recognising a known person based on the face, mapping, localisation, robust navigation in a known map. Deadline: 7.5.2014
\item MW2 - Increase robustness of face recognition, detection if a person is following the robot, speech recognition. Deadline: 27.7.2014
\item MW3 - Integration and test, producing new video. Deadline: 31.7.2014
\item MW4 - Cloth recognition, robot behaviour to restrict person to follow it. Deadline 31.8.2014
\item MW5 - Integration and test. Deadline: 5.9.2014
\item MW6 - \textbf{Perform complete \textit{Welcoming visitor} task during British science festival 6.9.-11.9.2014}
\item MW7 - Perform more test. Deadline 30.9.2014
\item MG1 - Work on autonomous recognition of state of doors, cooperation with human in detection of other changes. Deadline 31.10.2014
\item Test of overall system. Deadline 23.11.2014
\end{itemize}
The expected timetable of our milestones can be seen in Fig. \ref{fig:plan}.

%\begin{figure}[!htb]
%\centering
%\includegraphics[width=3.in]{timetable.png}
%\caption{Timetable}
%\label{fig:plan}
%\end{figure}
